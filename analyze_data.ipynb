{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime, timedelta\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pytz\n",
    "import glob\n",
    "import matplotlib.dates as mdates\n",
    "import os\n",
    "import pyarrow.parquet as pq\n",
    "import pyarrow as pa\n",
    "from dotenv import load_dotenv\n",
    "from tqdm import tqdm\n",
    "\n",
    "load_dotenv(\"config.env\")\n",
    "\n",
    "\n",
    "DATA_FOLDER = os.getenv(\"DATA_FOLDER\")\n",
    "OUTPUT_FOLDER = os.getenv(\"OUTPUT_FOLDER\")\n",
    "PLOTS_FOLDER = os.getenv(\"PLOTS_FOLDER\")\n",
    "PLOT_TIMEZONE = os.getenv(\"PLOT_TIMEZONE\")\n",
    "TIMEZONE = os.getenv(\"TIMEZONE\")\n",
    "SAMPLED_FOLDER = os.getenv(\"SAMPLED_FOLDER\")\n",
    "SAMPLE_FOLDER = os.getenv(\"SAMPLE_FOLDER\")\n",
    "GROUPED_DATA_FOLDER = os.getenv(\"GROUPED_DATA_FOLDER\")\n",
    "try:\n",
    "    BEACON_RATE = float(os.getenv(\"BEACON_RATE\"))\n",
    "except ValueError:\n",
    "    print(\"Invalid float value in BEACON_RATE env variable.\")\n",
    "    BEACON_RATE = 0.1024\n",
    "\n",
    "\n",
    "\n",
    "if DATA_FOLDER is None or OUTPUT_FOLDER is None or SAMPLE_FOLDER is None:\n",
    "    raise ValueError(\"Please set the environment variables DATA_FOLDER, OUTPUT_FOLDER, and SAMPLE_FOLDER.\")\n",
    "if not os.path.exists(OUTPUT_FOLDER):\n",
    "    os.makedirs(OUTPUT_FOLDER)\n",
    "if not os.path.exists(PLOTS_FOLDER):\n",
    "    os.makedirs(PLOTS_FOLDER)\n",
    "if not os.path.exists(SAMPLED_FOLDER):\n",
    "    os.makedirs(SAMPLED_FOLDER)\n",
    "if not os.path.exists(SAMPLE_FOLDER):\n",
    "    os.makedirs(SAMPLE_FOLDER)\n",
    "if not os.path.exists(GROUPED_DATA_FOLDER):\n",
    "    os.makedirs(GROUPED_DATA_FOLDER)\n",
    "# Check if the required folders exist, and create them if they don't\n",
    "\n",
    "\n",
    "# Initialize the global DataFrame for aggregate analysis\n",
    "all_class_beacons_df = pd.DataFrame()\n",
    "\n",
    "#log to a file and print to terminal\n",
    "log_file = f\"{OUTPUT_FOLDER}/log.txt\" #placeholder\n",
    "def log_and_print(message):\n",
    "    #print(message)  # Print to terminal is too noisy\n",
    "    log_file.write(message + \"\\n\")  # Write to file\n",
    "\n",
    "# # populate total station count for all SSIDs per radio\n",
    "def calculate_total_scount(df):\n",
    "\n",
    "    # Get unique wlan.ta and radio_number pairs\n",
    "    radio_1_transmitters = df[df[\"radio_number\"] == 1][\"wlan.ta\"].drop_duplicates().tolist()\n",
    "    radio_2_transmitters = df[df[\"radio_number\"] == 2][\"wlan.ta\"].drop_duplicates().tolist()\n",
    "\n",
    "    # Initialize the total_scount column\n",
    "    if \"total_scount\" not in df.columns:\n",
    "        df[\"total_scount\"] = 0\n",
    "\n",
    "\n",
    "    # Iterate over the rows to calculate total_scount\n",
    "    for i in range(len(df)):\n",
    "        current_ta = df.at[i, \"wlan.ta\"]\n",
    "        current_scount = df.at[i, \"wlan.qbss.scount\"]\n",
    "        current_radio = df.at[i, \"radio_number\"]\n",
    "\n",
    "        # set transmitter list based on radio\n",
    "        if current_radio == 1:\n",
    "             radio_transmitters = radio_1_transmitters.copy()\n",
    "        elif current_radio == 2:\n",
    "             radio_transmitters = radio_2_transmitters.copy()\n",
    "        else:\n",
    "            log_and_print(\"error - no radio\")\n",
    "\n",
    "        # remove the current line radio from the list\n",
    "        radio_transmitters.remove(current_ta)\n",
    "        \n",
    "        #create loop for every entry in radio_x_transmitters\n",
    "        for transmitter in radio_transmitters:\n",
    "             #find the next value for this transmitter and add it to current_scount\n",
    "\n",
    "            # Look for a next record where wlan.ta matches the one in the list\n",
    "            j = i + 1  # Start with the next row\n",
    "            while j < (len(df)):\n",
    "                next_ta = df.at[j, \"wlan.ta\"]\n",
    "                if next_ta == transmitter:\n",
    "                    # Found a record matching the ta\n",
    "                    next_scount = df.at[j, \"wlan.qbss.scount\"]\n",
    "                    current_scount += next_scount\n",
    "                    break\n",
    "                j += 1  # Go to the next row\n",
    "\n",
    "            # If no valid next record was found, look backwards\n",
    "            if j >= (len(df)):\n",
    "                k = i - 1  # Start with the previous row\n",
    "                while k > 0:\n",
    "                    prev_ta = df.at[k, \"wlan.ta\"]\n",
    "                    if prev_ta == transmitter:\n",
    "                        # Found the next record with the same wlan.ta_int\n",
    "                        prev_scount = df.at[k, \"wlan.qbss.scount\"]\n",
    "                        current_scount += prev_scount\n",
    "                        break\n",
    "                    k -= 1\n",
    "            \n",
    "            # If no next record exists, keep total_scount as current_scount\n",
    "        #update the value after all is added\n",
    "        df.at[i, \"total_scount\"] = current_scount\n",
    "    \n",
    "    # Ensure total_scount is of type integer\n",
    "    df[\"total_scount\"] = df[\"total_scount\"].astype(int)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "#  Define the function to generate the plot and save it as a file\n",
    "#inputs are dataframes filtered to the radio, and further filtered for each SSID\n",
    "def plot_channel_stations(radio_df, eduroam_df, byu_wifi_df, AP_name, start_time, end_time):\n",
    "    #print(\"plot_channel_stations\")\n",
    "    plt.figure(figsize=(14, 7))  # Example size: 12 inches wide, 6 inches tall\n",
    "    # Add a horizontal line at value 75 with 20% opacity\n",
    "    #plt.axhline(y=75, color='red', linestyle='-', alpha=0.2, zorder=-1)\n",
    "\n",
    "    # Access the first record and get the value of 'radio_number'\n",
    "    plot_radio_number = radio_df.iloc[0][\"radio_number\"]\n",
    "    \n",
    "    # Plot wlan.qbss.cu normalized as percentages\n",
    "    plt.plot(radio_df[\"aruba_erm.time\"], (radio_df[\"wlan.qbss.cu\"] / 255) * 100, label=\"Channel Utilization (%)\")\n",
    "\n",
    "    # Fill the area below the line\n",
    "    plt.fill_between(radio_df[\"aruba_erm.time\"], (radio_df[\"wlan.qbss.cu\"] / 255) * 100)\n",
    "\n",
    "\n",
    "    # Plot total_scount\n",
    "    plt.plot(radio_df[\"aruba_erm.time\"], radio_df[\"total_scount\"], label=\"Total Station Count\")\n",
    "\n",
    "    # not graphing these, but it may be interesting later\n",
    "    # Plot total_scount for \"eduroam\"\n",
    "    #plt.plot(eduroam_df[\"aruba_erm.time\"], eduroam_df[\"wlan.qbss.scount\"], label=\"eduroam Station Count\")\n",
    "\n",
    "    # Plot total_scount for \"BYU-WiFi\"\n",
    "    #plt.plot(byu_wifi_df[\"aruba_erm.time\"], byu_wifi_df[\"wlan.qbss.scount\"], label=\"BYU-WiFi Station Count\")\n",
    "\n",
    "    # Plot total_adc\n",
    "    #plt.plot(radio_df[\"aruba_erm.time\"], radio_df[\"wlan.qbss.adc\"] / 32767 * 100, label=\"ADC (%)\")\n",
    "\n",
    "    #plot adc% + cu\n",
    "    #adc updates less frequently than cu, but seems to be well correlated\n",
    "    #plt.plot(radio_df[\"aruba_erm.time\"], 100 - ((radio_df[\"wlan.qbss.adc\"] / 32767 * 100) + (radio_df[\"wlan.qbss.cu\"] / 255 * 100)), label=\"ADC + CU\")\n",
    "\n",
    "    formatted_date = start_time.strftime(\"%Y-%m-%d\")\n",
    "    formatted_start = start_time.strftime(\"%H:%M\")  # Keep date and time (no seconds or timezone)\n",
    "    formatted_end = end_time.strftime(\"%H:%M\")  # Keep only the time (no date, seconds, or timezone)\n",
    "\n",
    "    plt.xlabel(f\"Time ({formatted_date})\", fontsize=14)\n",
    "\n",
    "    # Add labels and title\n",
    "    #plt.xlabel(f\"{start_time}        to        {end_time}\", fontsize=14)\n",
    "    plt.ylabel(\"Values\", fontsize=14)\n",
    "    plt.title(f\"{AP_name} Radio {plot_radio_number} {formatted_start} to {formatted_end}\", fontsize=16)\n",
    "\n",
    "    # Set the y-axis limits from 0 to 100\n",
    "    plt.ylim(0, 100)\n",
    "\n",
    "    # Format x-axis to show only time (hour:minute)\n",
    "    # Define Time zone\n",
    "    time_zone = pytz.timezone(PLOT_TIMEZONE) # update to use env file\n",
    "    # Set the date formatter with time zone\n",
    "    time_formatter = mdates.DateFormatter(\"%H:%M\", tz=time_zone)\n",
    "    plt.gca().xaxis.set_major_formatter(time_formatter)\n",
    "\n",
    "    # Add a legend\n",
    "    plt.legend()\n",
    "\n",
    "    # Save the plot as a PNG file\n",
    "    plot_file = PLOTS_FOLDER + \"/\" + AP_name + \"_\" + start_time.strftime(\"%Y-%m-%d_%H-%M\") + \"_R\" + str(plot_radio_number) + \".png\" #change to use env file\n",
    "    #print(plot_file)\n",
    "\n",
    "    #save to a file\n",
    "    plt.savefig(plot_file)\n",
    "    \n",
    "    plt.close()\n",
    "    #print(plot_file)\n",
    "    # Display the plot\n",
    "    #plt.show()\n",
    "    return plot_file\n",
    "\n",
    "\n",
    "# Function to perform the analysis for a given radio DataFrame\n",
    "def analyze_radio_group(radio_df, radio_number):\n",
    "    #print(\"analyze_radio_group\")\n",
    "    #need to add > 127 count etc.\n",
    "    # Ensure the DataFrame is sorted by timestamp\n",
    "    radio_df = radio_df.sort_values(by=\"aruba_erm.time\").reset_index(drop=True)\n",
    "\n",
    "    # Count the total number of data points\n",
    "    total_data_points = len(radio_df)\n",
    "\n",
    "    # Calculate the greatest value of wlan.qbss.cu\n",
    "    greatest_cutil = ((radio_df[\"wlan.qbss.cu\"].max() / 255) * 100)\n",
    "\n",
    "    # Calculate the least value of wlan.qbss.cu\n",
    "    least_cutil = ((radio_df[\"wlan.qbss.cu\"].min() / 255) * 100)\n",
    "\n",
    "    # Count the number of data points where wlan.qbss.cu is greater than 191 (75%)\n",
    "    greater_than_191_count = (radio_df[\"wlan.qbss.cu\"] > 191).sum()\n",
    "\n",
    "    # Calculate the percentage of data points where wlan.qbss.cu is greater than 191\n",
    "    percentage_greater_than_191 = (greater_than_191_count / total_data_points) * 100\n",
    "\n",
    "    # Median wlan.qbss.cu\n",
    "    median_cutil = ((radio_df[\"wlan.qbss.cu\"].median() / 255) * 100)\n",
    "\n",
    "    # Calculate the greatest value of wlan.qbss.scount\n",
    "    greatest_scount = radio_df[\"total_scount\"].max()\n",
    "\n",
    "    # Calculate the median value of wlan.qbss.scount\n",
    "    median_scount = radio_df[\"total_scount\"].median()\n",
    "\n",
    "    # Calculate the least value of wlan.qbss.scount\n",
    "    least_scount = radio_df[\"total_scount\"].min()\n",
    "\n",
    "    # Print the results\n",
    "    log_and_print(\"\")\n",
    "    log_and_print(f\"--- Radio Number {radio_number} ---\")\n",
    "    log_and_print(f\"Total beacons: {total_data_points}\")\n",
    "    log_and_print(f\"Number of beacons with high channel utilization: {greater_than_191_count}\")\n",
    "    log_and_print(f\"Percentage of beacons with high channel utilization: {percentage_greater_than_191:.2f}%\")\n",
    "    log_and_print(f\"Highest channel utilization: {greatest_cutil:.2f}%\")\n",
    "    log_and_print(f\"Median channel utilization: {median_cutil:.2f}%\")\n",
    "    log_and_print(f\"Lowest channel utilization: {least_cutil:.2f}%\")\n",
    "    log_and_print(f\"Highest station count: {greatest_scount:.0f}\")\n",
    "    log_and_print(f\"Median station count: {median_scount:.0f}\")\n",
    "    log_and_print(f\"Lowest station count: {least_scount:.0f}\")\n",
    "\n",
    "    # Create a boolean mask for wlan.qbss.cu > 191\n",
    "    condition = radio_df[\"wlan.qbss.cu\"] > 191\n",
    "\n",
    "    # Assign group numbers for consecutive periods where condition is True\n",
    "    radio_df[\"group\"] = (condition != condition.shift()).cumsum() * condition\n",
    "\n",
    "    # Filter groups where wlan.qbss.cu > 191\n",
    "    valid_groups = radio_df[radio_df[\"group\"] > 0]\n",
    "\n",
    "    # Check if there are valid groups (i.e., groups where wlan.qbss.cu > 191)\n",
    "    if valid_groups.empty:\n",
    "        log_and_print(f\"No records found where wlan.qbss.cu > 191 for radio_number {radio_number}.\")\n",
    "        longest_duration = 0\n",
    "    else:\n",
    "       # Find the start and end of each group\n",
    "        group_details = valid_groups.groupby(\"group\").agg(\n",
    "            first_index=(\"aruba_erm.time\", \"idxmin\"),\n",
    "            last_index=(\"aruba_erm.time\", \"idxmax\")\n",
    "        )\n",
    "\n",
    "        # Extract the first and last timestamps directly\n",
    "        group_details[\"first_in_sequence_time\"] = valid_groups.groupby(\"group\")[\"aruba_erm.time\"].min()\n",
    "        group_details[\"end_time\"] = valid_groups.groupby(\"group\")[\"aruba_erm.time\"].max()\n",
    "\n",
    "        # Calculate the duration between the start and end times, and add 1 second \n",
    "        # Aruba updates beacons once per 10, which is also the rate that we have captured. 1.024 is the time the first beacon represents\n",
    "        group_details[\"duration\"] = (group_details[\"end_time\"] - group_details[\"first_in_sequence_time\"]).dt.total_seconds() + BEACON_RATE # need to add env variable\n",
    "\n",
    "        # Find the group with the longest duration\n",
    "        longest_group = group_details[\"duration\"].idxmax()\n",
    "        longest_row = group_details.loc[longest_group]\n",
    "\n",
    "        # Extract details of the longest duration\n",
    "        longest_duration = longest_row[\"duration\"]\n",
    "        longest_start_time = longest_row[\"first_in_sequence_time\"]\n",
    "        longest_end_time = longest_row[\"end_time\"]\n",
    "\n",
    "        # Print results for the specific radio_number\n",
    "        log_and_print(f\"Longest consecutive duration (streak) of high channel use: {longest_duration:.2f} seconds\")\n",
    "        log_and_print(f\"First beacon of sequence: {longest_start_time}\")\n",
    "        log_and_print(f\"Last beacon of sequence: {longest_end_time}\")\n",
    "\n",
    "    return total_data_points, greater_than_191_count, percentage_greater_than_191, greatest_cutil, median_cutil, least_cutil, greatest_scount, median_scount, least_scount, longest_duration\n",
    "\n",
    "\n",
    "# aggregate all beacons to a single dataframe for analysis of those during class sessions\n",
    "# this is periodically written, and is faster than writing to a parquet file often\n",
    "def add_to_all_class_beacons(filtered_df):\n",
    "    global all_class_beacons_df\n",
    "    all_class_beacons_df = pd.concat([all_class_beacons_df, filtered_df], ignore_index=True)\n",
    "    #log_and_print(\"ran add_to_all_class_beacons\")\n",
    "\n",
    "\n",
    "# clean up the parquet files further\n",
    "# keep only ~1 beacon per second\n",
    "# Function to filter rows for each wlan.ta\n",
    "def filter_by_time(df):\n",
    "    df = df.sort_values(\"aruba_erm.time\")  # Ensure sorted order\n",
    "    keep_rows = [df.iloc[0]]  # Always keep the first row\n",
    "    \n",
    "    last_time = df.iloc[0][\"aruba_erm.time\"]\n",
    "    for _, row in df.iloc[1:].iterrows():\n",
    "        if (row[\"aruba_erm.time\"] - last_time).total_seconds() >= (0.95 * BEACON_RATE): # keep ones that are more than 95 TU apart\n",
    "            keep_rows.append(row)\n",
    "            last_time = row[\"aruba_erm.time\"]\n",
    "    \n",
    "    return pd.DataFrame(keep_rows)\n",
    "\n",
    "#receives the dataframe for an AP, sorts it, and sends it to be filtered by time\n",
    "def remove_duplicate_beacons(ap_df):\n",
    "\n",
    "    # Group by 'wlan.ta' without dropping it\n",
    "    # has deprecation warning, can probably just sort by time then wlan.ta without grouping since the times will be different between different tas\n",
    "    filtered_ap_df = (\n",
    "        ap_df.groupby(\"wlan.ta\", group_keys=False, observed=True)\n",
    "        .apply(lambda group: filter_by_time(group.reset_index())) #was apply(filter_by_time)\n",
    "        .reset_index(drop=True)  # Reset index to avoid multi-index issues\n",
    "    )\n",
    "\n",
    "    # Convert specific columns back to string\n",
    "    string_columns = [\"wlan.ta\", \"wlan.vs.aruba.ap_name\", \"wlan.ssid\"]\n",
    "    filtered_ap_df[string_columns] = filtered_ap_df[string_columns].astype(\"string\")\n",
    "\n",
    "\n",
    "    return(filtered_ap_df)\n",
    "\n",
    "\n",
    "# Receives the information for each AP and class time information\n",
    "# Reads the file and processes the information for that time/AP\n",
    "def graph(ap_name, start_time, end_time, location, course, enrolled, capacity, ap_count, ap_df, radio_summary_df):\n",
    "\n",
    "    #log_and_print(\"\")\n",
    "    log_and_print(\"\\n-----------------------------------------------------------------\\n\")\n",
    "    log_and_print(f\"AP name: {ap_name}\")\n",
    "\n",
    "    # copy the AP df for manipulation\n",
    "    filtered_df = ap_df.copy() # this is needed, otherwise the original df is modified\n",
    "\n",
    "    # time column\n",
    "    filtered_df.reset_index(inplace=True) #reset index so that the timestamp is in a column instead\n",
    "\n",
    "    # Fix timestamp consistency\n",
    "    # Localize only if the timestamps are naive\n",
    "    if filtered_df[\"aruba_erm.time\"].dt.tz is None:\n",
    "        filtered_df[\"aruba_erm.time\"] = filtered_df[\"aruba_erm.time\"].dt.tz_localize(\"UTC\")\n",
    "\n",
    "    # Convert to UTC-7\n",
    "    filtered_df[\"aruba_erm.time\"] = filtered_df[\"aruba_erm.time\"].dt.tz_convert(TIMEZONE) # update to use env file\n",
    "\n",
    "    # Filter the DataFrame by the date/time range\n",
    "    filtered_df = filtered_df[(filtered_df['aruba_erm.time'] >= start_time) & (filtered_df['aruba_erm.time'] <= end_time)]\n",
    "\n",
    "    # return if there's no data\n",
    "    if filtered_df.empty:\n",
    "        log_and_print(f\"No data available for {ap_name} from {start_time} to {end_time}.\")\n",
    "        return\n",
    "\n",
    "    filtered_df = remove_duplicate_beacons(filtered_df)\n",
    "\n",
    "    # Ensure the DataFrame is sorted by timestamp\n",
    "    filtered_df = filtered_df.sort_values(by=\"aruba_erm.time\").reset_index(drop=True)\n",
    "\n",
    "\n",
    "    # Convert wlan.qbss.scount to integer, handling NaN values\n",
    "    filtered_df[\"wlan.qbss.scount\"] = filtered_df[\"wlan.qbss.scount\"].fillna(0).astype(int)\n",
    "    filtered_df[\"wlan.qbss.cu\"] = filtered_df[\"wlan.qbss.cu\"].fillna(0).astype(int)\n",
    "    filtered_df[\"wlan.qbss.adc\"] = filtered_df[\"wlan.qbss.adc\"].fillna(0).astype(int)\n",
    "\n",
    "    # create wlan.ta_int column\n",
    "    # Function to convert hex MAC address string to integer\n",
    "    def mac_to_int(mac):\n",
    "        return int(mac.replace(\":\", \"\"), 16)\n",
    "\n",
    "    # Apply the conversion to the wlan.ta column\n",
    "    filtered_df[\"wlan.ta_int\"] = filtered_df[\"wlan.ta\"].apply(mac_to_int)\n",
    "    \n",
    "    \n",
    "\n",
    "    # Check Radio and SSID count\n",
    "    # Verify all values in wlan.vs.aruba.ap_name are the same\n",
    "    if filtered_df[\"wlan.vs.aruba.ap_name\"].nunique() != 1:\n",
    "        raise ValueError(\"Values in wlan.vs.aruba.ap_name are not all the same.\")\n",
    "    #else:\n",
    "        #wlan_ap_name_value = filtered_df[\"wlan.vs.aruba.ap_name\"].iloc[0]\n",
    "        #print(f\"Data from {wlan_ap_name_value}\")\n",
    "        \n",
    "\n",
    "\n",
    "    # Count unique transmit addresses and corresponding SSIDs\n",
    "    # ensures that data is clean and shows multi-radio APs\n",
    "    # Group by wlan.ta and collect the corresponding wlan.ssid values\n",
    "    #bssid_info = (\n",
    "    #    filtered_df.groupby(\"wlan.ta\")[\"wlan.ssid\"]\n",
    "    #    .unique()  # Get unique SSIDs for each wlan.ta\n",
    "    #    .reset_index()\n",
    "        #.rename(columns={\"wlan.ta\": \"transmit_address\", \"wlan.ssid\": \"SSIDs\"})\n",
    "    #)\n",
    "\n",
    "    # Count the number of unique BSSIDs, SSIDs\n",
    "    #unique_bssid_count = bssid_info[\"wlan.ta\"].nunique()\n",
    "    bssid_count = filtered_df[\"wlan.ta\"].nunique() #bssid count for all radios until divided below\n",
    "    ssid_count = filtered_df[\"wlan.ssid\"].nunique()\n",
    "\n",
    "\n",
    "\n",
    "    # add a column for unique radio number\n",
    "    # this would be better with channel numbers than BSSIDs\n",
    "    # Create the 'radio_number' column\n",
    "    filtered_df = filtered_df.sort_values(\"wlan.ta_int\").reset_index(drop=True)\n",
    "\n",
    "    # Initialize the 'radio_number' column\n",
    "    radio_number = 1\n",
    "    radio_numbers = [radio_number]\n",
    "\n",
    "    # Assign radio numbers based on 'wlan.ta_int'\n",
    "    # would probably be better to use channel number, but this works for Aruba\n",
    "    for i in range(1, len(filtered_df)):\n",
    "        # Check if the difference between consecutive wlan.ta_int values is greater than 1\n",
    "        if abs(filtered_df.loc[i, \"wlan.ta_int\"] - filtered_df.loc[i - 1, \"wlan.ta_int\"]) > 1:\n",
    "            radio_number += 1  # Increment radio number if the condition is met\n",
    "        radio_numbers.append(radio_number)\n",
    "\n",
    "    # Add the radio_number column to the DataFrame\n",
    "    filtered_df[\"radio_number\"] = radio_numbers\n",
    "\n",
    "    # Ensure the DataFrame is sorted by timestamp\n",
    "    filtered_df = filtered_df.sort_values(by=\"aruba_erm.time\").reset_index(drop=True)\n",
    "\n",
    "    # Apply the function to calculate total_scount\n",
    "    filtered_df = calculate_total_scount(filtered_df)\n",
    "\n",
    "    # send the df to aggregate function\n",
    "    add_to_all_class_beacons(filtered_df)\n",
    "\n",
    "    #print number of radios\n",
    "    radio_count = filtered_df['radio_number'].max()\n",
    "    #print(f\"Number or 5GHz radios: {filtered_df['radio_number'].max()}\")\n",
    "    log_and_print(f\"Number or 5GHz radios: {radio_count}\")\n",
    "    #bssid_count = bssid_count / radio_count if radio_count != 0 else 0 # assumes that all radios have the same number of BSSIDs\n",
    "\n",
    "    # Output the BSSID results\n",
    "    log_and_print(f\"Number of BSSIDs: {bssid_count}\")\n",
    "    log_and_print(f\"Number of SSIDs: {ssid_count}\")\n",
    "    #print(\"List of addresses and their corresponding SSIDs:\")\n",
    "    #print(bssid_info)\n",
    "\n",
    "    # Filter the DataFrame for records where radio_number is 1\n",
    "    filtered_radio_1_df = filtered_df[filtered_df[\"radio_number\"] == 1]\n",
    "    # Filter the data for \"eduroam\" and \"BYU-WiFi\" SSIDs\n",
    "    #eduroam_radio_1_df = filtered_df[(filtered_df[\"wlan.ssid\"] == \"eduroam\") & (filtered_df[\"radio_number\"] == 1)]\n",
    "    #byu_wifi_radio_1_df = filtered_df[(filtered_df[\"wlan.ssid\"] == \"BYU-WiFi\") & (filtered_df[\"radio_number\"] == 1)]\n",
    "    eduroam_radio_1_df = None # this part not used\n",
    "    byu_wifi_radio_1_df = None # not used, uncomment to graph SSID station count\n",
    "\n",
    "    #  Filter the DataFrame for records where radio_number is 2\n",
    "    filtered_radio_2_df = filtered_df[filtered_df[\"radio_number\"] == 2]\n",
    "\n",
    "    # Run the analysis for radio_number 1 (using filtered_radio_1_df)\n",
    "    if not filtered_radio_1_df.empty:\n",
    "        total_beacons, high_cu_beacons, percent_high_cu, highest_cu, median_cu, lowest_cu, high_scount, median_scount, low_scount, longest_duration_high_cu = analyze_radio_group(filtered_radio_1_df, 1)\n",
    "        # Plot for the first radio\n",
    "        #temp removed for performance\n",
    "        #plot_file = \"\"\n",
    "        plot_file = plot_channel_stations(filtered_radio_1_df, eduroam_radio_1_df, byu_wifi_radio_1_df, ap_name, start_time, end_time)\n",
    "        #print(plot_file)\n",
    "        #update totals summary\n",
    "        radio_summary_df.loc[len(radio_summary_df)] = [\n",
    "        course, location, ap_name, 1, start_time, end_time,\n",
    "        capacity, enrolled, ap_count, radio_count, bssid_count, ssid_count, total_beacons,\n",
    "        high_cu_beacons, percent_high_cu, highest_cu, median_cu, lowest_cu,\n",
    "        high_scount, median_scount, low_scount, longest_duration_high_cu, plot_file\n",
    "        ]\n",
    "\n",
    "    # Run the analysis for radio_number 2 (using filtered_radio_2_df) only if it's not empty\n",
    "    if not filtered_radio_2_df.empty:\n",
    "        total_beacons, high_cu_beacons, percent_high_cu, highest_cu, median_cu, lowest_cu, high_scount, median_scount, low_scount, longest_duration_high_cu = analyze_radio_group(filtered_radio_2_df, 2)\n",
    "\n",
    "\n",
    "        # Filter the data for \"eduroam\" and \"BYU-WiFi\" SSIDs\n",
    "        #eduroam_radio_2_df = filtered_df[(filtered_df[\"wlan.ssid\"] == \"eduroam\") & (filtered_df[\"radio_number\"] == 2)] #update to use env file\n",
    "        #byu_wifi_radio_2_df = filtered_df[(filtered_df[\"wlan.ssid\"] == \"BYU-WiFi\") & (filtered_df[\"radio_number\"] == 2)] #update to use env file\n",
    "        eduroam_radio_2_df = None\n",
    "        byu_wifi_radio_2_df = None # unused, change to graph SSIDs\n",
    "        # Plot for the second radio\n",
    "        #temp remove for performance\n",
    "        #plot_file = \"\"\n",
    "        plot_file = plot_channel_stations(filtered_radio_2_df, eduroam_radio_2_df, byu_wifi_radio_2_df, ap_name, start_time, end_time)\n",
    "        #print(plot_file)\n",
    "        #update totals summary\n",
    "        radio_summary_df.loc[len(radio_summary_df)] = [\n",
    "        course, location, ap_name, 2, start_time, end_time,\n",
    "        capacity, enrolled, ap_count, radio_count, bssid_count, ssid_count, total_beacons,\n",
    "        high_cu_beacons, percent_high_cu, highest_cu, median_cu, lowest_cu,\n",
    "        high_scount, median_scount, low_scount, longest_duration_high_cu, plot_file\n",
    "        ]\n",
    "    else:\n",
    "        log_and_print(f\"{ap_name} has only one 5GHz radio\")\n",
    "\n",
    "    #print()\n",
    "    \n",
    "    # return df not needed since it is directly modified\n",
    "    #return radio_summary_df\n",
    "\n",
    "\n",
    "# Write to parquet files for each AP that can be combined later\n",
    "# all_class_beacons is a global variable that is updated by the add_to_all_class_beacons function called in the graph function\n",
    "def append_to_parquet(AP_name, all_class_beacons_df):\n",
    "    # Ensure all_class_beacons_df is not empty before appending\n",
    "    if not all_class_beacons_df.empty:\n",
    "        ap_beacons_file = f\"Data/sampled/{AP_name}_class_beacons.parquet\"\n",
    "        \n",
    "        # Define schema based on the DataFrame\n",
    "        schema = pa.Schema.from_pandas(all_class_beacons_df)\n",
    "\n",
    "        # Check if the file exists\n",
    "        if os.path.exists(ap_beacons_file):\n",
    "            # If file exists, open it and append using ParquetWriter\n",
    "            existing_table = pq.read_table(ap_beacons_file)\n",
    "            with pq.ParquetWriter(ap_beacons_file, schema, compression=\"snappy\") as writer:\n",
    "                # Append the existing data first\n",
    "                writer.write_table(existing_table)\n",
    "                # Append the new data\n",
    "                table = pa.Table.from_pandas(all_class_beacons_df, schema=schema)\n",
    "                writer.write_table(table)\n",
    "            log_and_print(f\"Adding to {ap_beacons_file}\")\n",
    "        else:\n",
    "            # If the file doesn't exist, create it and write data\n",
    "            all_class_beacons_df.to_parquet(ap_beacons_file, engine=\"pyarrow\", compression=\"snappy\", index=False)\n",
    "            log_and_print(f\"Creating {ap_beacons_file}\")\n",
    "        \n",
    "    #else:\n",
    "        #log_and_print(\"DataFrame is empty, nothing to append.\")\n",
    "\n",
    "\n",
    "# combine all files in Data/sampled into one file\n",
    "def combine_all_class_beacons():\n",
    "    all_class_beacons_df = pd.DataFrame()\n",
    "\n",
    "    # Directory containing AP .parquet files\n",
    "    directory = SAMPLED_FOLDER\n",
    "\n",
    "    # Get a list of all .parquet files\n",
    "    parquet_files = glob.glob(directory + \"/*.parquet\")\n",
    "\n",
    "    all_class_beacons_df = all_class_beacons_df.iloc[0:0] #clear contents of the df\n",
    "    # Read and concatenate all Parquet files (faster than .pkl)\n",
    "    all_class_beacons_df = pd.concat([pd.read_parquet(file) for file in parquet_files], ignore_index=True)\n",
    "\n",
    "    all_class_beacons_df.to_parquet(f'{DATA_FOLDER}/all_class_beacons.parquet')\n",
    "\n",
    "\n",
    "# main function to process all data in sample files\n",
    "\n",
    "def main():\n",
    "    \n",
    "    #read in sample files\n",
    "    # this can be done after parquet files have been created from samples.py\n",
    "    sample_classes_df = pd.read_parquet(f'{SAMPLE_FOLDER}/sample_classes.parquet')\n",
    "    sample_aps_df = pd.read_parquet(f'{SAMPLE_FOLDER}/sample_aps.parquet')\n",
    "    sample_classrooms_df = pd.read_parquet(f'{SAMPLE_FOLDER}/sample_classrooms.parquet')\n",
    "\n",
    "    process(sample_aps_df, sample_classes_df, sample_classrooms_df)\n",
    "\n",
    "    log_and_print(\"\\n-----------------------------------------------------------------\\n\")\n",
    "    log_and_print(f\"Analysis completed on {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    combine_all_class_beacons()\n",
    "    log_and_print(f\"all_class_beacons combined on {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    log_file.close()\n",
    "\n",
    "\n",
    "\n",
    "def process(sample_aps_df, sample_classes_df, sample_classrooms_df):\n",
    "    # will loop for each class row, iterate through each AP at that location\n",
    "    # this will go through ALL data and take a while\n",
    "\n",
    "    global all_class_beacons_df\n",
    "\n",
    "    #create a separate dataframe for aggregate radio information\n",
    "    # Define column names and data types\n",
    "    columns = {\n",
    "        \"subject\": str,\n",
    "        \"location\": str,\n",
    "        \"AP_hostname\": str,\n",
    "        \"radio_number\": int,\n",
    "        \"start_time\": \"datetime64[ns]\",\n",
    "        \"end_time\": \"datetime64[ns]\",\n",
    "        \"capacity\": int,\n",
    "        \"enrolled\": int,\n",
    "        \"AP_count\": int,\n",
    "        \"radio_count\": int,\n",
    "        \"bssid_count\": int,\n",
    "        \"ssid_count\": int,\n",
    "        \"total_beacons\": int,\n",
    "        \"high_cu_beacons\": int,\n",
    "        \"percent_high_cu\": float,\n",
    "        \"highest_cu\": int,\n",
    "        \"median_cu\": int,\n",
    "        \"lowest_cu\": int,\n",
    "        \"high_scount\": int,\n",
    "        \"median_scount\": int,\n",
    "        \"low_scount\": int,\n",
    "        \"longest_duration_high_cu\": float,\n",
    "        \"graph_filename\": str\n",
    "    }\n",
    "\n",
    "    # Create an empty DataFrame with specified columns and data types\n",
    "    radio_summary_df = pd.DataFrame({col: pd.Series(dtype=dtype) for col, dtype in columns.items()})\n",
    "\n",
    "    # Display the empty DataFrame\n",
    "    #print(radio_summary_df.head)\n",
    "    \n",
    "\n",
    "    # Initialize the global DataFrame for aggregate analysis\n",
    "    #all_class_beacons_df = pd.DataFrame() # moved to top\n",
    "\n",
    "    # Define the base directory for the files\n",
    "    base_dir = GROUPED_DATA_FOLDER\n",
    "\n",
    "    # Initialize variables to ensure it's defined\n",
    "    ap_df = pd.DataFrame()\n",
    "    AP_name = \"\"\n",
    "\n",
    "\n",
    "    # Loop through each row in sample_classes_df\n",
    "    for index, class_row in tqdm(sample_classes_df.iterrows(), total=len(sample_classes_df)):\n",
    "        location = class_row['location']\n",
    "        start_time = class_row['start_time']\n",
    "        end_time = class_row['end_time']\n",
    "        course = class_row['course']\n",
    "        enrolled = class_row['enrolled']\n",
    "        capacity = class_row['capacity']\n",
    "        ap_count = sample_classrooms_df.loc[sample_classrooms_df['Location'] == location, 'ap_count'].iloc[0]\n",
    "\n",
    "\n",
    "        log_and_print(\"\\n-----------------------------------------------------------------\")\n",
    "        log_and_print(\"-----------------------------------------------------------------\\n\")\n",
    "        log_and_print(f\"Subject: {course}\")\n",
    "        log_and_print(f\"Location: {location}\")\n",
    "        log_and_print(f\"Start Time: {start_time}\")\n",
    "        log_and_print(f\"End Time: {end_time}\")\n",
    "        log_and_print(f\"Capacity: {capacity}\")\n",
    "        log_and_print(f\"Enrolled: {enrolled}\")\n",
    "        log_and_print(f\"Number of APs: {ap_count}\")\n",
    "        log_and_print(\"\")\n",
    "        \n",
    "        # Sub loop for each matching AP in sample_aps_df\n",
    "        for ap_index, ap_row in sample_aps_df.iterrows():\n",
    "            building_room = ap_row['Building-Room']\n",
    "            \n",
    "            # Check if the building-room matches the location\n",
    "            if building_room == location:\n",
    "                    \n",
    "                if AP_name != ap_row['Hostname']:\n",
    "                    AP_name = ap_row['Hostname']\n",
    "                    file_name = AP_name + \".parquet\" #temp using cleaned up files\n",
    "                    # Load the data for the AP file into a dataframe\n",
    "                    ap_df = pd.read_parquet(\n",
    "                        f\"{base_dir}/{file_name}\",\n",
    "                        engine=\"pyarrow\",\n",
    "                    )\n",
    "                \n",
    "                # Call the graph function with the variables (do not need to set to a variable in order to update radio_summary_df)\n",
    "                # Graph function will remove duplicates, sort, clean the data, add to all_class_beacons_df, calculate station counts, create plots, and do some analysis\n",
    "                graph(AP_name, start_time, end_time, location, course, enrolled, capacity, ap_count, ap_df, radio_summary_df)\n",
    "            #write to parquet file\n",
    "            append_to_parquet(AP_name, all_class_beacons_df) #writes the file for the AP\n",
    "            all_class_beacons_df = all_class_beacons_df.iloc[0:0]  # Clear contents of the df, otherwise it would build to become all\n",
    "\n",
    "    # write summary dataframe to a file so it can be analyzed further later\n",
    "    radio_summary_df.to_parquet(f'{DATA_FOLDER}/radio_summary.parquet')\n",
    "\n",
    "\n",
    "\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run\n",
    "if __name__ == \"__main__\":\n",
    "    timestamp = datetime.now().strftime(\"%Y-%m-%d_%H-%M\")\n",
    "    log_filename = f\"{OUTPUT_FOLDER}/analyze_data_log_{timestamp}.txt\" #logging - need to change to logging function\n",
    "    log_file = open(log_filename, \"a\")  # Open file in append mode\n",
    "    log_and_print(f\"Analysis started on {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Beacon-analysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
